{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XingweiLiang/XingweiLiang.github.io/blob/main/icefall_yes_no_dataset_recipe_with_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC0w8FMdN1j1"
      },
      "source": [
        "# Yesno recipe in icefall\n",
        "\n",
        "This notebook shows you how to setup the environment to use [icefall][icefall] for training and decoding.\n",
        "It also describes how to use a per-trained model to decode waves.\n",
        "\n",
        "\n",
        "We use the [yesno] dataset as an example.\n",
        "\n",
        "[icefall]: https://github.com/k2-fsa/icefall\n",
        "[yesno]: https://www.openslr.org/1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8jMkYeDQUeY"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etfavk8FQXRQ"
      },
      "source": [
        "### Install PyTorch and torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AliAaueDNteG",
        "outputId": "1ecd0bed-f1ee-4e21-f8b9-3c1d4def5bd1"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXyznh4VaZVj"
      },
      "source": [
        "Colab pre-installs PyTorch, so we don't need to install it here.\n",
        "\n",
        "From https://pytorch.org/audio/main/installation.html#compatibility-matrix, we need to install torchaudio==2.0.2 as the current PyTorch version is 2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJzBhtJAQbD6",
        "outputId": "ad70bc9c-5ec6-4ab9-87e9-a1b4364d6ee0"
      },
      "source": [
        "! pip install torchaudio==2.0.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio==2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio==2.0.2) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio==2.0.2) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchaudio==2.0.2) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchaudio==2.0.2) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchaudio==2.0.2) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchaudio==2.0.2) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0HY88GiR7_J"
      },
      "source": [
        "### Install k2\n",
        "\n",
        "We are going to install k2 by following https://k2-fsa.github.io/k2/installation/from_wheels.html.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JccZFPBQ7vJ",
        "outputId": "397e5d7f-f9b8-4a9f-cb91-bc098b148bf7"
      },
      "source": [
        "! pip install k2==1.24.3.dev20230718+cuda11.8.torch2.0.1 -f https://k2-fsa.github.io/k2/cuda.html"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://k2-fsa.github.io/k2/cuda.html\n",
            "Collecting k2==1.24.3.dev20230718+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/k2/resolve/main/cuda/k2-1.24.3.dev20230718%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.9/117.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.0.1+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->k2==1.24.3.dev20230718+cuda11.8.torch2.0.1) (1.3.0)\n",
            "Installing collected packages: k2\n",
            "Successfully installed k2-1.24.3.dev20230718+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jl3lHExSTSI"
      },
      "source": [
        "Check that k2 was installed successfully:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYnIIeh6SPz9",
        "outputId": "8db3bbac-d6b8-4732-efe2-b7921bb1e08b"
      },
      "source": [
        "! python3 -m k2.version"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting environment information...\n",
            "\n",
            "k2 version: 1.24.3\n",
            "Build type: Release\n",
            "Git SHA1: e400fa3b456faf8afe0ee5bfe572946b4921a3db\n",
            "Git date: Sat Jul 15 04:21:50 2023\n",
            "Cuda used to build k2: 11.8\n",
            "cuDNN used to build k2: \n",
            "Python version used to build k2: 3.10\n",
            "OS used to build k2: CentOS Linux release 7.9.2009 (Core)\n",
            "CMake version: 3.26.4\n",
            "GCC version: 9.3.1\n",
            "CMAKE_CUDA_FLAGS:  -Wno-deprecated-gpu-targets   -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_35,code=sm_35  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_50,code=sm_50  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_60,code=sm_60  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_61,code=sm_61  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_70,code=sm_70  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_75,code=sm_75  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_80,code=sm_80  -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w  --expt-extended-lambda -gencode arch=compute_86,code=sm_86 -DONNX_NAMESPACE=onnx_c2 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_89,code=compute_89 -gencode arch=compute_90,code=compute_90 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=integer_sign_change,--diag_suppress=useless_using_declaration,--diag_suppress=set_but_not_used,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=implicit_return_from_non_void_function,--diag_suppress=unsigned_compare_with_zero,--diag_suppress=declared_but_not_referenced,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda -D_GLIBCXX_USE_CXX11_ABI=0 --compiler-options -Wall  --compiler-options -Wno-strict-overflow  --compiler-options -Wno-unknown-pragmas \n",
            "CMAKE_CXX_FLAGS:  -D_GLIBCXX_USE_CXX11_ABI=0 -Wno-unused-variable  -Wno-strict-overflow \n",
            "PyTorch version used to build k2: 2.0.1+cu118\n",
            "PyTorch is using Cuda: 11.8\n",
            "NVTX enabled: True\n",
            "With CUDA: True\n",
            "Disable debug: True\n",
            "Sync kernels : False\n",
            "Disable checks: False\n",
            "Max cpu memory allocate: 214748364800 bytes (or 200.0 GB)\n",
            "k2 abort: False\n",
            "__file__: /usr/local/lib/python3.10/dist-packages/k2/version/version.py\n",
            "_k2.__file__: /usr/local/lib/python3.10/dist-packages/_k2.cpython-310-x86_64-linux-gnu.so\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biN4HMqFSdJ5"
      },
      "source": [
        "### Install lhotse\n",
        "[lhotse][lhotse] is used for data preparation.\n",
        "\n",
        "[lhotse]: https://github.com/lhotse-speech/lhotse\n",
        "\n",
        "Normally, we would use `pip install lhotse`. However, the yesno recipe is added recently and has not been released to PyPI yet, so we install the latest unreleased version here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6SsFZwCESWz_",
        "outputId": "bf7d18f7-c7e2-44f6-8d95-afbbaf2251bb"
      },
      "source": [
        "# ! pip install lhotse\n",
        "! pip install git+https://github.com/lhotse-speech/lhotse"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/lhotse-speech/lhotse\n",
            "  Cloning https://github.com/lhotse-speech/lhotse to /tmp/pip-req-build-ayzs7262\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lhotse-speech/lhotse /tmp/pip-req-build-ayzs7262\n",
            "  Resolved https://github.com/lhotse-speech/lhotse to commit 287de1dbadcb70bf7398d3d482c1f730d2c5407f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (3.0.1)\n",
            "Requirement already satisfied: SoundFile>=0.10 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (0.12.1)\n",
            "Requirement already satisfied: click>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (8.1.7)\n",
            "Collecting cytoolz>=0.10.1 (from lhotse==1.17.0.dev0+git.287de1d.clean)\n",
            "  Downloading cytoolz-0.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses (from lhotse==1.17.0.dev0+git.287de1d.clean)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting intervaltree>=3.1.0 (from lhotse==1.17.0.dev0+git.287de1d.clean)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (6.0.1)\n",
            "Requirement already satisfied: tabulate>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (4.66.1)\n",
            "Collecting lilcom>=1.1.0 (from lhotse==1.17.0.dev0+git.287de1d.clean)\n",
            "  Downloading lilcom-1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.1/87.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from lhotse==1.17.0.dev0+git.287de1d.clean) (2.0.2+cu118)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->lhotse==1.17.0.dev0+git.287de1d.clean) (0.12.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3.1.0->lhotse==1.17.0.dev0+git.287de1d.clean) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10->lhotse==1.17.0.dev0+git.287de1d.clean) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.17.0.dev0+git.287de1d.clean) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.17.0.dev0+git.287de1d.clean) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.17.0.dev0+git.287de1d.clean) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.17.0.dev0+git.287de1d.clean) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.17.0.dev0+git.287de1d.clean) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->lhotse==1.17.0.dev0+git.287de1d.clean) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->lhotse==1.17.0.dev0+git.287de1d.clean) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->lhotse==1.17.0.dev0+git.287de1d.clean) (17.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10->lhotse==1.17.0.dev0+git.287de1d.clean) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->lhotse==1.17.0.dev0+git.287de1d.clean) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->lhotse==1.17.0.dev0+git.287de1d.clean) (1.3.0)\n",
            "Building wheels for collected packages: lhotse, intervaltree\n",
            "  Building wheel for lhotse (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lhotse: filename=lhotse-1.17.0.dev0+git.287de1d.clean-py3-none-any.whl size=729005 sha256=ad242482c2cbf0a22c3283b62898ef448ec7407eb4f7690af2ba748080d52ced\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lh1e2ab6/wheels/df/b0/ff/cce0f16868fcdbee2088f3acf9f249dc90117d5f5dd9b6f69d\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26094 sha256=5673ff1afbbb3e519ff16b39d62dcf58a2dcb8e04e3af837fdf0f04fea499739\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built lhotse intervaltree\n",
            "Installing collected packages: dataclasses, lilcom, intervaltree, cytoolz, lhotse\n",
            "Successfully installed cytoolz-0.12.2 dataclasses-0.6 intervaltree-3.1.0 lhotse-1.17.0.dev0+git.287de1d.clean lilcom-1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwq8uAGpSyzu"
      },
      "source": [
        "### Install icefall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ivIXM8FS0Yy"
      },
      "source": [
        "[icefall][icefall] is a collection of Python scripts.\n",
        "You don't need to install it. What you need to do is\n",
        "to get its source code, install its dependencies, and\n",
        "set the `PYTHONPATH` pointing to it.\n",
        "\n",
        "[icefall]: https://github.com/k2-fsa/icefall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aflfLSytSnFe",
        "outputId": "53156d15-eb81-4e76-902a-037feae292f3"
      },
      "source": [
        "! pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP4RZ31xTKzL",
        "outputId": "95f22a45-4b89-4d08-bb90-738c24a31280"
      },
      "source": [
        "! git clone https://github.com/k2-fsa/icefall"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'icefall'...\n",
            "remote: Enumerating objects: 14310, done.\u001b[K\n",
            "remote: Counting objects: 100% (1427/1427), done.\u001b[K\n",
            "remote: Compressing objects: 100% (524/524), done.\u001b[K\n",
            "remote: Total 14310 (delta 892), reused 1237 (delta 801), pack-reused 12883\u001b[K\n",
            "Receiving objects: 100% (14310/14310), 16.58 MiB | 20.13 MiB/s, done.\n",
            "Resolving deltas: 100% (9720/9720), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1c5zESwTtii"
      },
      "source": [
        "Now install dependencies of `icefall`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYOwkp6FTrdH",
        "outputId": "37d0354d-b671-4c9e-899d-a2ab1fbd7cb1"
      },
      "source": [
        "! cd icefall && \\\n",
        "  pip install -r requirements.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaldifst in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.7.5)\n",
            "Requirement already satisfied: kaldilm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.15.1)\n",
            "Requirement already satisfied: kaldialign in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.7.2)\n",
            "Requirement already satisfied: kaldi-decoder in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.2.2)\n",
            "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.1.99)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.13.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.3.7)\n",
            "Requirement already satisfied: black==22.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (22.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black==22.3.0->-r requirements.txt (line 9)) (8.1.7)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black==22.3.0->-r requirements.txt (line 9)) (3.11.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black==22.3.0->-r requirements.txt (line 9)) (0.11.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black==22.3.0->-r requirements.txt (line 9)) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black==22.3.0->-r requirements.txt (line 9)) (2.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.4.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (0.41.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from typeguard->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 6)) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 6)) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 6)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeFyXWEOT4Mi"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqMH7ZLKT8jM"
      },
      "source": [
        "We have set up the environment. Now it is the time to prepare data for training and decoding.\n",
        "\n",
        "As we just said, `icefall` is a collection of Python scripts and we have to set up the `PYTHONPATH` variable to use it. Remember that `icefall` was downloaded to\n",
        "`/content/icefall`, so we use\n",
        "\n",
        "```\n",
        "export PYTHONPATH=/content/icefall:$PYTHONPATH\n",
        "```\n",
        "\n",
        "**HINT**: You can have several versions of `icefall` in your virtual environemnt. To switch to a specific version of `icefall`, just change the `PYTHONPATH` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To remove the following warning message\n",
        "# 2023-07-27 05:03:07.156920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "! pip uninstall -y tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YQUEXWruxFu",
        "outputId": "08148968-0c36-4809-9eeb-4271cea6e4dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.13.0\n",
            "Uninstalling tensorflow-2.13.0:\n",
            "  Successfully uninstalled tensorflow-2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzyw8VyfUjUB",
        "outputId": "a4f34e50-d698-45fb-d8a8-b98af0777fc5"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  rm -rf data && \\\n",
        "  ./prepare.sh"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-06 05:10:49 (prepare.sh:27:main) dl_dir: /content/icefall/egs/yesno/ASR/download\n",
            "2023-10-06 05:10:49 (prepare.sh:30:main) Stage 0: Download data\n",
            "/content/icefall/egs/yesno/ASR/download/waves_yesno.tar.gz: 100% 4.70M/4.70M [00:01<00:00, 4.01MB/s]\n",
            "2023-10-06 05:10:54 (prepare.sh:39:main) Stage 1: Prepare yesno manifest\n",
            "2023-10-06 05:10:57 (prepare.sh:45:main) Stage 2: Compute fbank for yesno\n",
            "2023-10-06 05:11:00,052 INFO [compute_fbank_yesno.py:65] Processing train\n",
            "Extracting and storing features: 100% 90/90 [00:00<00:00, 168.90it/s]\n",
            "2023-10-06 05:11:00,610 INFO [compute_fbank_yesno.py:65] Processing test\n",
            "Extracting and storing features: 100% 30/30 [00:00<00:00, 293.60it/s]\n",
            "2023-10-06 05:11:01 (prepare.sh:51:main) Stage 3: Prepare lang\n",
            "2023-10-06 05:11:07,249 INFO [prepare_lang_fst.py:174] Building standard CTC topology\n",
            "2023-10-06 05:11:07,250 INFO [prepare_lang_fst.py:183] Building L\n",
            "2023-10-06 05:11:07,250 INFO [prepare_lang_fst.py:191] Building HL\n",
            "2023-10-06 05:11:07,251 INFO [prepare_lang_fst.py:201] Skip building HLG\n",
            "2023-10-06 05:11:07 (prepare.sh:67:main) Stage 4: Prepare G\n",
            "/project/kaldilm/csrc/arpa_file_parser.cc:void kaldilm::ArpaFileParser::Read(std::istream&):79\n",
            "[I] Reading \\data\\ section.\n",
            "/project/kaldilm/csrc/arpa_file_parser.cc:void kaldilm::ArpaFileParser::Read(std::istream&):140\n",
            "[I] Reading \\1-grams: section.\n",
            "2023-10-06 05:11:07 (prepare.sh:93:main) Stage 5: Compile HLG\n",
            "2023-10-06 05:11:09,695 INFO [compile_hlg.py:124] Processing data/lang_phone\n",
            "2023-10-06 05:11:09,696 INFO [lexicon.py:171] Converting L.pt to Linv.pt\n",
            "2023-10-06 05:11:09,698 INFO [compile_hlg.py:48] Building ctc_topo. max_token_id: 3\n",
            "2023-10-06 05:11:09,699 INFO [compile_hlg.py:52] Loading G.fst.txt\n",
            "2023-10-06 05:11:09,699 INFO [compile_hlg.py:62] Intersecting L and G\n",
            "2023-10-06 05:11:09,700 INFO [compile_hlg.py:64] LG shape: (4, None)\n",
            "2023-10-06 05:11:09,700 INFO [compile_hlg.py:66] Connecting LG\n",
            "2023-10-06 05:11:09,701 INFO [compile_hlg.py:68] LG shape after k2.connect: (4, None)\n",
            "2023-10-06 05:11:09,701 INFO [compile_hlg.py:70] <class 'torch.Tensor'>\n",
            "2023-10-06 05:11:09,701 INFO [compile_hlg.py:71] Determinizing LG\n",
            "2023-10-06 05:11:09,702 INFO [compile_hlg.py:74] <class '_k2.ragged.RaggedTensor'>\n",
            "2023-10-06 05:11:09,702 INFO [compile_hlg.py:76] Connecting LG after k2.determinize\n",
            "2023-10-06 05:11:09,702 INFO [compile_hlg.py:79] Removing disambiguation symbols on LG\n",
            "2023-10-06 05:11:09,703 INFO [compile_hlg.py:91] LG shape after k2.remove_epsilon: (6, None)\n",
            "2023-10-06 05:11:09,704 INFO [compile_hlg.py:96] Arc sorting LG\n",
            "2023-10-06 05:11:09,704 INFO [compile_hlg.py:99] Composing H and LG\n",
            "2023-10-06 05:11:09,704 INFO [compile_hlg.py:106] Connecting LG\n",
            "2023-10-06 05:11:09,704 INFO [compile_hlg.py:109] Arc sorting LG\n",
            "2023-10-06 05:11:09,705 INFO [compile_hlg.py:111] HLG.shape: (8, None)\n",
            "2023-10-06 05:11:09,705 INFO [compile_hlg.py:127] Saving HLG.pt to data/lang_phone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RAnFhhqZgPo"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1phegInRZkbl",
        "outputId": "2379f61a-1e2c-4a7e-9b73-94a354f9daf2"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/train.py"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-06 05:12:19,770 INFO [train.py:481] Training started\n",
            "2023-10-06 05:12:19,770 INFO [train.py:482] {'exp_dir': PosixPath('tdnn/exp'), 'lang_dir': PosixPath('data/lang_phone'), 'lr': 0.01, 'feature_dim': 23, 'weight_decay': 1e-06, 'start_epoch': 0, 'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 10, 'reset_interval': 20, 'valid_interval': 10, 'beam_size': 10, 'reduction': 'sum', 'use_double_scores': True, 'world_size': 1, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 15, 'seed': 42, 'feature_dir': PosixPath('data/fbank'), 'max_duration': 30.0, 'bucketing_sampler': False, 'num_buckets': 10, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': False, 'return_cuts': True, 'num_workers': 2, 'env_info': {'k2-version': '1.24.3', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'e400fa3b456faf8afe0ee5bfe572946b4921a3db', 'k2-git-date': 'Sat Jul 15 04:21:50 2023', 'lhotse-version': '1.17.0.dev+git.287de1d.clean', 'torch-version': '2.0.1+cu118', 'torch-cuda-available': True, 'torch-cuda-version': '11.8', 'python-version': '3.1', 'icefall-git-branch': 'master', 'icefall-git-sha1': '109354b-clean', 'icefall-git-date': 'Mon Oct 2 06:00:06 2023', 'icefall-path': '/content/icefall', 'k2-path': '/usr/local/lib/python3.10/dist-packages/k2/__init__.py', 'lhotse-path': '/usr/local/lib/python3.10/dist-packages/lhotse/__init__.py', 'hostname': 'd1dc439066c8', 'IP address': '172.28.0.12'}}\n",
            "2023-10-06 05:12:19,772 INFO [lexicon.py:168] Loading pre-compiled data/lang_phone/Linv.pt\n",
            "2023-10-06 05:12:19,774 INFO [train.py:495] device: cuda:0\n",
            "2023-10-06 05:12:25,288 INFO [asr_datamodule.py:146] About to get train cuts\n",
            "2023-10-06 05:12:25,289 INFO [asr_datamodule.py:245] About to get train cuts\n",
            "2023-10-06 05:12:25,610 INFO [asr_datamodule.py:149] About to create train dataset\n",
            "2023-10-06 05:12:25,610 INFO [asr_datamodule.py:199] Using SimpleCutSampler.\n",
            "2023-10-06 05:12:25,610 INFO [asr_datamodule.py:205] About to create train dataloader\n",
            "2023-10-06 05:12:25,611 INFO [asr_datamodule.py:218] About to get test cuts\n",
            "2023-10-06 05:12:25,611 INFO [asr_datamodule.py:253] About to get test cuts\n",
            "2023-10-06 05:12:33,639 INFO [train.py:422] Epoch 0, batch 0, loss[loss=1.065, over 2436.00 frames. ], tot_loss[loss=1.065, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:34,419 INFO [train.py:422] Epoch 0, batch 10, loss[loss=0.4583, over 2828.00 frames. ], tot_loss[loss=0.7086, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:35,107 INFO [train.py:444] Epoch 0, validation loss=0.9226, over 18067.00 frames. \n",
            "2023-10-06 05:12:35,827 INFO [train.py:422] Epoch 0, batch 20, loss[loss=0.2603, over 2695.00 frames. ], tot_loss[loss=0.4857, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:36,323 INFO [train.py:444] Epoch 0, validation loss=0.4741, over 18067.00 frames. \n",
            "2023-10-06 05:12:36,387 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-0.pt\n",
            "2023-10-06 05:12:36,473 INFO [train.py:422] Epoch 1, batch 0, loss[loss=0.2567, over 2436.00 frames. ], tot_loss[loss=0.2567, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:37,123 INFO [train.py:422] Epoch 1, batch 10, loss[loss=0.1216, over 2828.00 frames. ], tot_loss[loss=0.1641, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:37,597 INFO [train.py:444] Epoch 1, validation loss=0.1423, over 18067.00 frames. \n",
            "2023-10-06 05:12:38,145 INFO [train.py:422] Epoch 1, batch 20, loss[loss=0.0733, over 2695.00 frames. ], tot_loss[loss=0.1204, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:38,456 INFO [train.py:444] Epoch 1, validation loss=0.06941, over 18067.00 frames. \n",
            "2023-10-06 05:12:38,494 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-1.pt\n",
            "2023-10-06 05:12:38,552 INFO [train.py:422] Epoch 2, batch 0, loss[loss=0.07953, over 2436.00 frames. ], tot_loss[loss=0.07953, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:38,985 INFO [train.py:422] Epoch 2, batch 10, loss[loss=0.04598, over 2828.00 frames. ], tot_loss[loss=0.05488, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:39,288 INFO [train.py:444] Epoch 2, validation loss=0.03915, over 18067.00 frames. \n",
            "2023-10-06 05:12:39,705 INFO [train.py:422] Epoch 2, batch 20, loss[loss=0.03378, over 2695.00 frames. ], tot_loss[loss=0.04666, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:40,012 INFO [train.py:444] Epoch 2, validation loss=0.0329, over 18067.00 frames. \n",
            "2023-10-06 05:12:40,048 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-2.pt\n",
            "2023-10-06 05:12:40,106 INFO [train.py:422] Epoch 3, batch 0, loss[loss=0.03582, over 2436.00 frames. ], tot_loss[loss=0.03582, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:40,531 INFO [train.py:422] Epoch 3, batch 10, loss[loss=0.02439, over 2828.00 frames. ], tot_loss[loss=0.02822, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:40,845 INFO [train.py:444] Epoch 3, validation loss=0.02405, over 18067.00 frames. \n",
            "2023-10-06 05:12:41,284 INFO [train.py:422] Epoch 3, batch 20, loss[loss=0.02222, over 2695.00 frames. ], tot_loss[loss=0.02704, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:41,623 INFO [train.py:444] Epoch 3, validation loss=0.02539, over 18067.00 frames. \n",
            "2023-10-06 05:12:41,665 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-3.pt\n",
            "2023-10-06 05:12:41,726 INFO [train.py:422] Epoch 4, batch 0, loss[loss=0.02164, over 2436.00 frames. ], tot_loss[loss=0.02164, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:42,159 INFO [train.py:422] Epoch 4, batch 10, loss[loss=0.01616, over 2828.00 frames. ], tot_loss[loss=0.01876, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:42,468 INFO [train.py:444] Epoch 4, validation loss=0.0175, over 18067.00 frames. \n",
            "2023-10-06 05:12:42,869 INFO [train.py:422] Epoch 4, batch 20, loss[loss=0.01799, over 2695.00 frames. ], tot_loss[loss=0.01925, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:43,179 INFO [train.py:444] Epoch 4, validation loss=0.01814, over 18067.00 frames. \n",
            "2023-10-06 05:12:43,229 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-4.pt\n",
            "2023-10-06 05:12:43,288 INFO [train.py:422] Epoch 5, batch 0, loss[loss=0.01612, over 2436.00 frames. ], tot_loss[loss=0.01612, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:43,726 INFO [train.py:422] Epoch 5, batch 10, loss[loss=0.01309, over 2828.00 frames. ], tot_loss[loss=0.01487, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:44,028 INFO [train.py:444] Epoch 5, validation loss=0.01488, over 18067.00 frames. \n",
            "2023-10-06 05:12:44,427 INFO [train.py:422] Epoch 5, batch 20, loss[loss=0.01456, over 2695.00 frames. ], tot_loss[loss=0.01526, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:44,742 INFO [train.py:444] Epoch 5, validation loss=0.01428, over 18067.00 frames. \n",
            "2023-10-06 05:12:44,779 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-5.pt\n",
            "2023-10-06 05:12:44,840 INFO [train.py:422] Epoch 6, batch 0, loss[loss=0.01401, over 2436.00 frames. ], tot_loss[loss=0.01401, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:45,266 INFO [train.py:422] Epoch 6, batch 10, loss[loss=0.01128, over 2828.00 frames. ], tot_loss[loss=0.01258, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:45,565 INFO [train.py:444] Epoch 6, validation loss=0.01351, over 18067.00 frames. \n",
            "2023-10-06 05:12:45,973 INFO [train.py:422] Epoch 6, batch 20, loss[loss=0.01336, over 2695.00 frames. ], tot_loss[loss=0.01436, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:46,271 INFO [train.py:444] Epoch 6, validation loss=0.01345, over 18067.00 frames. \n",
            "2023-10-06 05:12:46,307 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-6.pt\n",
            "2023-10-06 05:12:46,367 INFO [train.py:422] Epoch 7, batch 0, loss[loss=0.01261, over 2436.00 frames. ], tot_loss[loss=0.01261, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:46,790 INFO [train.py:422] Epoch 7, batch 10, loss[loss=0.01051, over 2828.00 frames. ], tot_loss[loss=0.01189, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:47,108 INFO [train.py:444] Epoch 7, validation loss=0.01226, over 18067.00 frames. \n",
            "2023-10-06 05:12:47,523 INFO [train.py:422] Epoch 7, batch 20, loss[loss=0.0133, over 2695.00 frames. ], tot_loss[loss=0.01297, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:47,820 INFO [train.py:444] Epoch 7, validation loss=0.01199, over 18067.00 frames. \n",
            "2023-10-06 05:12:47,857 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-7.pt\n",
            "2023-10-06 05:12:47,913 INFO [train.py:422] Epoch 8, batch 0, loss[loss=0.01183, over 2436.00 frames. ], tot_loss[loss=0.01183, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:48,519 INFO [train.py:422] Epoch 8, batch 10, loss[loss=0.00983, over 2828.00 frames. ], tot_loss[loss=0.01092, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:48,990 INFO [train.py:444] Epoch 8, validation loss=0.01199, over 18067.00 frames. \n",
            "2023-10-06 05:12:49,616 INFO [train.py:422] Epoch 8, batch 20, loss[loss=0.01245, over 2695.00 frames. ], tot_loss[loss=0.01202, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:50,107 INFO [train.py:444] Epoch 8, validation loss=0.01177, over 18067.00 frames. \n",
            "2023-10-06 05:12:50,165 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-8.pt\n",
            "2023-10-06 05:12:50,262 INFO [train.py:422] Epoch 9, batch 0, loss[loss=0.0118, over 2436.00 frames. ], tot_loss[loss=0.0118, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:50,918 INFO [train.py:422] Epoch 9, batch 10, loss[loss=0.009522, over 2828.00 frames. ], tot_loss[loss=0.01061, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:51,401 INFO [train.py:444] Epoch 9, validation loss=0.0115, over 18067.00 frames. \n",
            "2023-10-06 05:12:52,049 INFO [train.py:422] Epoch 9, batch 20, loss[loss=0.01214, over 2695.00 frames. ], tot_loss[loss=0.01165, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:52,506 INFO [train.py:444] Epoch 9, validation loss=0.01121, over 18067.00 frames. \n",
            "2023-10-06 05:12:52,560 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-9.pt\n",
            "2023-10-06 05:12:52,646 INFO [train.py:422] Epoch 10, batch 0, loss[loss=0.01097, over 2436.00 frames. ], tot_loss[loss=0.01097, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:53,115 INFO [train.py:422] Epoch 10, batch 10, loss[loss=0.009372, over 2828.00 frames. ], tot_loss[loss=0.01036, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:53,424 INFO [train.py:444] Epoch 10, validation loss=0.0111, over 18067.00 frames. \n",
            "2023-10-06 05:12:53,845 INFO [train.py:422] Epoch 10, batch 20, loss[loss=0.01233, over 2695.00 frames. ], tot_loss[loss=0.01088, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:54,159 INFO [train.py:444] Epoch 10, validation loss=0.01108, over 18067.00 frames. \n",
            "2023-10-06 05:12:54,199 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-10.pt\n",
            "2023-10-06 05:12:54,260 INFO [train.py:422] Epoch 11, batch 0, loss[loss=0.01079, over 2436.00 frames. ], tot_loss[loss=0.01079, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:54,695 INFO [train.py:422] Epoch 11, batch 10, loss[loss=0.009102, over 2828.00 frames. ], tot_loss[loss=0.01013, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:55,023 INFO [train.py:444] Epoch 11, validation loss=0.01218, over 18067.00 frames. \n",
            "2023-10-06 05:12:55,424 INFO [train.py:422] Epoch 11, batch 20, loss[loss=0.01183, over 2695.00 frames. ], tot_loss[loss=0.01114, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:55,735 INFO [train.py:444] Epoch 11, validation loss=0.01087, over 18067.00 frames. \n",
            "2023-10-06 05:12:55,772 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-11.pt\n",
            "2023-10-06 05:12:55,841 INFO [train.py:422] Epoch 12, batch 0, loss[loss=0.01064, over 2436.00 frames. ], tot_loss[loss=0.01064, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:56,269 INFO [train.py:422] Epoch 12, batch 10, loss[loss=0.00908, over 2828.00 frames. ], tot_loss[loss=0.009981, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:56,582 INFO [train.py:444] Epoch 12, validation loss=0.01089, over 18067.00 frames. \n",
            "2023-10-06 05:12:56,995 INFO [train.py:422] Epoch 12, batch 20, loss[loss=0.01178, over 2695.00 frames. ], tot_loss[loss=0.01102, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:57,302 INFO [train.py:444] Epoch 12, validation loss=0.01081, over 18067.00 frames. \n",
            "2023-10-06 05:12:57,340 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-12.pt\n",
            "2023-10-06 05:12:57,397 INFO [train.py:422] Epoch 13, batch 0, loss[loss=0.0105, over 2436.00 frames. ], tot_loss[loss=0.0105, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:57,891 INFO [train.py:422] Epoch 13, batch 10, loss[loss=0.009006, over 2828.00 frames. ], tot_loss[loss=0.009937, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:58,197 INFO [train.py:444] Epoch 13, validation loss=0.01278, over 18067.00 frames. \n",
            "2023-10-06 05:12:58,603 INFO [train.py:422] Epoch 13, batch 20, loss[loss=0.01172, over 2695.00 frames. ], tot_loss[loss=0.01058, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:12:58,915 INFO [train.py:444] Epoch 13, validation loss=0.01078, over 18067.00 frames. \n",
            "2023-10-06 05:12:58,952 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-13.pt\n",
            "2023-10-06 05:12:59,008 INFO [train.py:422] Epoch 14, batch 0, loss[loss=0.01045, over 2436.00 frames. ], tot_loss[loss=0.01045, over 2436.00 frames. ], batch size: 4\n",
            "2023-10-06 05:12:59,433 INFO [train.py:422] Epoch 14, batch 10, loss[loss=0.00897, over 2828.00 frames. ], tot_loss[loss=0.009857, over 22192.90 frames. ], batch size: 4\n",
            "2023-10-06 05:12:59,734 INFO [train.py:444] Epoch 14, validation loss=0.01081, over 18067.00 frames. \n",
            "2023-10-06 05:13:00,140 INFO [train.py:422] Epoch 14, batch 20, loss[loss=0.01169, over 2695.00 frames. ], tot_loss[loss=0.01044, over 34971.47 frames. ], batch size: 5\n",
            "2023-10-06 05:13:00,442 INFO [train.py:444] Epoch 14, validation loss=0.0109, over 18067.00 frames. \n",
            "2023-10-06 05:13:00,478 INFO [checkpoint.py:75] Saving checkpoint to tdnn/exp/epoch-14.pt\n",
            "2023-10-06 05:13:00,480 INFO [train.py:555] Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnwI18BKcq6e"
      },
      "source": [
        "## Decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WS-i2RdYzrx",
        "outputId": "55c05837-1526-4e75-fa4b-9aa91329cf9a"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/decode.py"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-06 05:13:25,509 INFO [decode.py:262] Decoding started\n",
            "2023-10-06 05:13:25,509 INFO [decode.py:263] {'exp_dir': PosixPath('tdnn/exp'), 'lang_dir': PosixPath('data/lang_phone'), 'feature_dim': 23, 'search_beam': 20, 'output_beam': 8, 'min_active_states': 30, 'max_active_states': 10000, 'use_double_scores': True, 'epoch': 14, 'avg': 2, 'export': False, 'feature_dir': PosixPath('data/fbank'), 'max_duration': 30.0, 'bucketing_sampler': False, 'num_buckets': 10, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 1.0, 'on_the_fly_feats': False, 'shuffle': False, 'return_cuts': True, 'num_workers': 2, 'env_info': {'k2-version': '1.24.3', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': 'e400fa3b456faf8afe0ee5bfe572946b4921a3db', 'k2-git-date': 'Sat Jul 15 04:21:50 2023', 'lhotse-version': '1.17.0.dev+git.287de1d.clean', 'torch-version': '2.0.1+cu118', 'torch-cuda-available': True, 'torch-cuda-version': '11.8', 'python-version': '3.1', 'icefall-git-branch': 'master', 'icefall-git-sha1': '109354b-clean', 'icefall-git-date': 'Mon Oct 2 06:00:06 2023', 'icefall-path': '/content/icefall', 'k2-path': '/usr/local/lib/python3.10/dist-packages/k2/__init__.py', 'lhotse-path': '/usr/local/lib/python3.10/dist-packages/lhotse/__init__.py', 'hostname': 'd1dc439066c8', 'IP address': '172.28.0.12'}}\n",
            "2023-10-06 05:13:25,510 INFO [lexicon.py:168] Loading pre-compiled data/lang_phone/Linv.pt\n",
            "2023-10-06 05:13:25,511 INFO [decode.py:272] device: cuda:0\n",
            "2023-10-06 05:13:27,204 INFO [decode.py:290] averaging ['tdnn/exp/epoch-13.pt', 'tdnn/exp/epoch-14.pt']\n",
            "2023-10-06 05:13:27,210 INFO [asr_datamodule.py:218] About to get test cuts\n",
            "2023-10-06 05:13:27,210 INFO [asr_datamodule.py:253] About to get test cuts\n",
            "2023-10-06 05:13:30,042 INFO [decode.py:203] batch 0/?, cuts processed until now is 4\n",
            "2023-10-06 05:13:31,722 INFO [decode.py:240] The transcripts are stored in tdnn/exp/recogs-test_set.txt\n",
            "2023-10-06 05:13:31,723 INFO [utils.py:629] [test_set] %WER 0.42% [1 / 240, 0 ins, 1 del, 0 sub ]\n",
            "2023-10-06 05:13:31,725 INFO [decode.py:248] Wrote detailed error stats to tdnn/exp/errs-test_set.txt\n",
            "2023-10-06 05:13:31,725 INFO [decode.py:315] Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMENow4cc53b"
      },
      "source": [
        "### Show the decoding result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yQT-VEJc3Xz",
        "outputId": "fca4d4c7-db3c-4cf2-a4a0-7c79f9790414"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  cat tdnn/exp/recogs-test_set.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0_0_0_1_0_0_0_1-0:\tref=['NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_0_1_0_0_0_1-0:\thyp=['NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_1_0_0_0_1_0-1:\tref=['NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO']\n",
            "0_0_1_0_0_0_1_0-1:\thyp=['NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO']\n",
            "0_0_1_0_0_1_1_1-2:\tref=['NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_0_1_0_0_1_1_1-2:\thyp=['NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_0_1_0_1_0_0_1-3:\tref=['NO', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'YES']\n",
            "0_0_1_0_1_0_0_1-3:\thyp=['NO', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'YES']\n",
            "0_0_1_1_0_0_0_1-4:\tref=['NO', 'NO', 'YES', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_1_1_0_0_0_1-4:\thyp=['NO', 'NO', 'YES', 'YES', 'NO', 'NO', 'NO', 'YES']\n",
            "0_0_1_1_0_1_1_0-5:\tref=['NO', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'NO']\n",
            "0_0_1_1_0_1_1_0-5:\thyp=['NO', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'NO']\n",
            "0_0_1_1_1_0_0_0-6:\tref=['NO', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "0_0_1_1_1_0_0_0-6:\thyp=['NO', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "0_0_1_1_1_1_0_0-7:\tref=['NO', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_0_1_1_1_1_0_0-7:\thyp=['NO', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_1_0_0_0_1_0_0-8:\tref=['NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO', 'NO']\n",
            "0_1_0_0_0_1_0_0-8:\thyp=['NO', 'YES', 'NO', 'NO', 'NO', 'YES', 'NO', 'NO']\n",
            "0_1_0_0_1_0_1_0-9:\tref=['NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "0_1_0_0_1_0_1_0-9:\thyp=['NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "0_1_0_1_0_0_0_0-10:\tref=['NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO']\n",
            "0_1_0_1_0_0_0_0-10:\thyp=['NO', 'YES', 'NO', 'YES', 'NO', 'NO', 'NO']\n",
            "0_1_0_1_1_1_0_0-11:\tref=['NO', 'YES', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_1_0_1_1_1_0_0-11:\thyp=['NO', 'YES', 'NO', 'YES', 'YES', 'YES', 'NO', 'NO']\n",
            "0_1_1_0_0_1_1_1-12:\tref=['NO', 'YES', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_1_1_0_0_1_1_1-12:\thyp=['NO', 'YES', 'YES', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "0_1_1_1_0_0_1_0-13:\tref=['NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "0_1_1_1_0_0_1_0-13:\thyp=['NO', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "0_1_1_1_1_0_1_0-14:\tref=['NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES', 'NO']\n",
            "0_1_1_1_1_0_1_0-14:\thyp=['NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES', 'NO']\n",
            "1_0_0_0_0_0_0_0-15:\tref=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO']\n",
            "1_0_0_0_0_0_0_0-15:\thyp=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO']\n",
            "1_0_0_0_0_0_1_1-16:\tref=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'YES']\n",
            "1_0_0_0_0_0_1_1-16:\thyp=['YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'YES']\n",
            "1_0_0_1_0_1_1_1-17:\tref=['YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_0_1_0_1_1_1-17:\thyp=['YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_1_1_0_1_1_1-18:\tref=['YES', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_1_1_0_1_1_1-18:\thyp=['YES', 'NO', 'YES', 'YES', 'NO', 'YES', 'YES', 'YES']\n",
            "1_0_1_1_1_1_0_1-19:\tref=['YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES']\n",
            "1_0_1_1_1_1_0_1-19:\thyp=['YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO', 'YES']\n",
            "1_1_0_0_0_1_1_1-20:\tref=['YES', 'YES', 'NO', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "1_1_0_0_0_1_1_1-20:\thyp=['YES', 'YES', 'NO', 'NO', 'NO', 'YES', 'YES', 'YES']\n",
            "1_1_0_0_1_0_1_1-21:\tref=['YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES']\n",
            "1_1_0_0_1_0_1_1-21:\thyp=['YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES', 'YES']\n",
            "1_1_0_1_0_1_0_0-22:\tref=['YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO']\n",
            "1_1_0_1_0_1_0_0-22:\thyp=['YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO', 'NO']\n",
            "1_1_0_1_1_0_0_1-23:\tref=['YES', 'YES', 'NO', 'YES', 'YES', 'NO', 'NO', 'YES']\n",
            "1_1_0_1_1_0_0_1-23:\thyp=['YES', 'YES', 'NO', 'YES', 'YES', 'NO', 'NO', 'YES']\n",
            "1_1_0_1_1_1_1_0-24:\tref=['YES', 'YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO']\n",
            "1_1_0_1_1_1_1_0-24:\thyp=['YES', 'YES', 'NO', 'YES', 'YES', 'YES', 'YES', 'NO']\n",
            "1_1_1_0_0_1_0_1-25:\tref=['YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES']\n",
            "1_1_1_0_0_1_0_1-25:\thyp=['YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO', 'YES']\n",
            "1_1_1_0_1_0_1_0-26:\tref=['YES', 'YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "1_1_1_0_1_0_1_0-26:\thyp=['YES', 'YES', 'YES', 'NO', 'YES', 'NO', 'YES', 'NO']\n",
            "1_1_1_1_0_0_1_0-27:\tref=['YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "1_1_1_1_0_0_1_0-27:\thyp=['YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'YES', 'NO']\n",
            "1_1_1_1_1_0_0_0-28:\tref=['YES', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "1_1_1_1_1_0_0_0-28:\thyp=['YES', 'YES', 'YES', 'YES', 'YES', 'NO', 'NO', 'NO']\n",
            "1_1_1_1_1_1_1_1-29:\tref=['YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES']\n",
            "1_1_1_1_1_1_1_1-29:\thyp=['YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES', 'YES']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RIxtJ-IdLob"
      },
      "source": [
        "### Show the detailed WER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lQFBS-KdIVx",
        "outputId": "29ef94f7-0414-4c89-c90c-75150fc66eed"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  cat tdnn/exp/errs-test_set.txt"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "%WER = 0.42\n",
            "Errors: 0 insertions, 1 deletions, 0 substitutions, over 240 reference words (239 correct)\n",
            "Search below for sections starting with PER-UTT DETAILS:, SUBSTITUTIONS:, DELETIONS:, INSERTIONS:, PER-WORD STATS:\n",
            "\n",
            "PER-UTT DETAILS: corr or (ref->hyp)  \n",
            "0_0_0_1_0_0_0_1-0:\tNO NO NO YES NO NO NO YES\n",
            "0_0_1_0_0_0_1_0-1:\tNO NO YES NO NO NO YES NO\n",
            "0_0_1_0_0_1_1_1-2:\tNO NO YES NO NO YES YES YES\n",
            "0_0_1_0_1_0_0_1-3:\tNO NO YES NO YES NO NO YES\n",
            "0_0_1_1_0_0_0_1-4:\tNO NO YES YES NO NO NO YES\n",
            "0_0_1_1_0_1_1_0-5:\tNO NO YES YES NO YES YES NO\n",
            "0_0_1_1_1_0_0_0-6:\tNO NO YES YES YES NO NO NO\n",
            "0_0_1_1_1_1_0_0-7:\tNO NO YES YES YES YES NO NO\n",
            "0_1_0_0_0_1_0_0-8:\tNO YES NO NO NO YES NO NO\n",
            "0_1_0_0_1_0_1_0-9:\tNO YES NO NO YES NO YES NO\n",
            "0_1_0_1_0_0_0_0-10:\tNO YES NO YES NO NO NO (NO->*)\n",
            "0_1_0_1_1_1_0_0-11:\tNO YES NO YES YES YES NO NO\n",
            "0_1_1_0_0_1_1_1-12:\tNO YES YES NO NO YES YES YES\n",
            "0_1_1_1_0_0_1_0-13:\tNO YES YES YES NO NO YES NO\n",
            "0_1_1_1_1_0_1_0-14:\tNO YES YES YES YES NO YES NO\n",
            "1_0_0_0_0_0_0_0-15:\tYES NO NO NO NO NO NO NO\n",
            "1_0_0_0_0_0_1_1-16:\tYES NO NO NO NO NO YES YES\n",
            "1_0_0_1_0_1_1_1-17:\tYES NO NO YES NO YES YES YES\n",
            "1_0_1_1_0_1_1_1-18:\tYES NO YES YES NO YES YES YES\n",
            "1_0_1_1_1_1_0_1-19:\tYES NO YES YES YES YES NO YES\n",
            "1_1_0_0_0_1_1_1-20:\tYES YES NO NO NO YES YES YES\n",
            "1_1_0_0_1_0_1_1-21:\tYES YES NO NO YES NO YES YES\n",
            "1_1_0_1_0_1_0_0-22:\tYES YES NO YES NO YES NO NO\n",
            "1_1_0_1_1_0_0_1-23:\tYES YES NO YES YES NO NO YES\n",
            "1_1_0_1_1_1_1_0-24:\tYES YES NO YES YES YES YES NO\n",
            "1_1_1_0_0_1_0_1-25:\tYES YES YES NO NO YES NO YES\n",
            "1_1_1_0_1_0_1_0-26:\tYES YES YES NO YES NO YES NO\n",
            "1_1_1_1_0_0_1_0-27:\tYES YES YES YES NO NO YES NO\n",
            "1_1_1_1_1_0_0_0-28:\tYES YES YES YES YES NO NO NO\n",
            "1_1_1_1_1_1_1_1-29:\tYES YES YES YES YES YES YES YES\n",
            "\n",
            "SUBSTITUTIONS: count ref -> hyp\n",
            "\n",
            "DELETIONS: count ref\n",
            "1   NO\n",
            "\n",
            "INSERTIONS: count hyp\n",
            "\n",
            "PER-WORD STATS: word  corr tot_errs count_in_ref count_in_hyp\n",
            "NO   115 1 116 115\n",
            "YES   124 0 124 124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lIChKjOr5J0"
      },
      "source": [
        "# Pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzfpY18xr-6P"
      },
      "source": [
        "### Download the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naBI_K9fr8Qn",
        "outputId": "01806fd2-ed11-43f2-c47c-50391b770f37"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  mkdir tmp && \\\n",
        "  cd tmp && \\\n",
        "  git lfs install && \\\n",
        "  git clone https://huggingface.co/csukuangfj/icefall_asr_yesno_tdnn"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'icefall_asr_yesno_tdnn'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Total 60 (delta 0), reused 0 (delta 0), pack-reused 60\u001b[K\n",
            "Unpacking objects: 100% (60/60), 2.28 MiB | 7.37 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hbcze8EsRcA",
        "outputId": "79a65278-9fb1-4fce-c18e-d97763bfec1f"
      },
      "source": [
        "! sudo apt-get install git-lfs"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCkos-5msW-K",
        "outputId": "56547737-e28f-468c-dd59-0792de7941f6"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  mkdir -p tmp && \\\n",
        "  tree tmp"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: tree: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufKWViBxsc41",
        "outputId": "6430295a-cd1d-4d84-b140-dd908ce4f693"
      },
      "source": [
        "! sudo apt-get install tree"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 1s (85.0 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 120879 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJn7iUw6se1W",
        "outputId": "3aff61be-5259-4e1a-ad5a-04e33cdd68fe"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  mkdir -p tmp && \\\n",
        "  tree tmp"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34mtmp\u001b[0m\n",
            "└── \u001b[01;34micefall_asr_yesno_tdnn\u001b[0m\n",
            "    ├── \u001b[01;34mlang_phone\u001b[0m\n",
            "    │   ├── \u001b[00mHLG.pt\u001b[0m\n",
            "    │   ├── \u001b[00mL_disambig.pt\u001b[0m\n",
            "    │   ├── \u001b[00mlexicon_disambig.txt\u001b[0m\n",
            "    │   ├── \u001b[00mlexicon.txt\u001b[0m\n",
            "    │   ├── \u001b[00mLinv.pt\u001b[0m\n",
            "    │   ├── \u001b[00mL.pt\u001b[0m\n",
            "    │   ├── \u001b[00mtokens.txt\u001b[0m\n",
            "    │   └── \u001b[00mwords.txt\u001b[0m\n",
            "    ├── \u001b[01;34mlm\u001b[0m\n",
            "    │   ├── \u001b[00mG.arpa\u001b[0m\n",
            "    │   └── \u001b[00mG.fst.txt\u001b[0m\n",
            "    ├── \u001b[00mpretrained.pt\u001b[0m\n",
            "    ├── \u001b[00mREADME.md\u001b[0m\n",
            "    └── \u001b[01;34mtest_waves\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_0_1_0_0_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_0_0_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_0_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_0_1_0_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_1_0_0_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_1_0_1_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_1_1_0_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_0_1_1_1_1_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_0_0_0_1_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_0_0_1_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_0_1_0_0_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_0_1_1_1_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_1_0_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_1_1_0_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m0_1_1_1_1_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_0_0_0_0_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_0_0_0_0_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_0_1_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_1_1_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_0_1_1_1_1_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_0_0_1_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_0_1_0_1_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_1_0_1_0_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_1_1_0_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_0_1_1_1_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_1_0_0_1_0_1.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_1_0_1_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_1_1_0_0_1_0.wav\u001b[0m\n",
            "        ├── \u001b[01;35m1_1_1_1_1_0_0_0.wav\u001b[0m\n",
            "        └── \u001b[01;35m1_1_1_1_1_1_1_1.wav\u001b[0m\n",
            "\n",
            "4 directories, 42 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8We8jsdsiPw",
        "outputId": "a5ee7e22-a2e6-45ef-980e-44919f298820"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  soxi tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: soxi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69CgbooXsorj",
        "outputId": "fd7ef84a-6ea1-468a-9a5e-23f4bc8a0b66"
      },
      "source": [
        "! sudo apt-get install sox"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base\n",
            "  libsox3 libwavpack1\n",
            "Suggested packages:\n",
            "  libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base\n",
            "  libsox3 libwavpack1 sox\n",
            "0 upgraded, 7 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 617 kB of archives.\n",
            "After this operation, 1,764 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [240 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [11.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [33.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 sox amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [104 kB]\n",
            "Fetched 617 kB in 1s (485 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 7.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 120887 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../2-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../3-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../4-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../6-sox_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRQ85XoTsqVq",
        "outputId": "3a73093b-c1d5-43fc-f56b-e7d271ab9e91"
      },
      "source": [
        "! cd /content/icefall/egs/yesno/ASR && \\\n",
        "  soxi tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input File     : 'tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav'\n",
            "Channels       : 1\n",
            "Sample Rate    : 8000\n",
            "Precision      : 16-bit\n",
            "Duration       : 00:00:06.76 = 54080 samples ~ 507 CDDA sectors\n",
            "File Size      : 108k\n",
            "Bit Rate       : 128k\n",
            "Sample Encoding: 16-bit Signed Integer PCM\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtEYXq6Lt4gH"
      },
      "source": [
        "## Download kaldifeat\n",
        "\n",
        "See https://csukuangfj.github.io/kaldifeat/installation/from_wheels.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yXGrYBbt7Xd",
        "outputId": "8bfb52ff-c21b-496d-f108-406e43d2746c"
      },
      "source": [
        "! pip install kaldifeat==1.25.0.dev20230726+cuda11.8.torch2.0.1  -f https://csukuangfj.github.io/kaldifeat/cuda.html"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://csukuangfj.github.io/kaldifeat/cuda.html\n",
            "Collecting kaldifeat==1.25.0.dev20230726+cuda11.8.torch2.0.1\n",
            "  Downloading https://huggingface.co/csukuangfj/kaldifeat/resolve/main/ubuntu-cuda/kaldifeat-1.25.0.dev20230726%2Bcuda11.8.torch2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (574 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m574.0/574.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaldifeat\n",
            "Successfully installed kaldifeat-1.25.0.dev20230726+cuda11.8.torch2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWmnk4YwsyQn"
      },
      "source": [
        "## Inference with a pre-trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-YdwbyfwS7F"
      },
      "source": [
        "### View help information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmU6_ZbLsvAg",
        "outputId": "05948e8f-92e7-45ee-ff48-619243815d39"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/pretrained.py --help"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: pretrained.py\n",
            "       [-h]\n",
            "       --checkpoint\n",
            "       CHECKPOINT\n",
            "       --words-file\n",
            "       WORDS_FILE\n",
            "       --HLG\n",
            "       HLG\n",
            "       sound_files\n",
            "       [sound_files ...]\n",
            "\n",
            "positional arguments:\n",
            "  sound_files\n",
            "    The input\n",
            "    sound\n",
            "    file(s) to\n",
            "    transcribe.\n",
            "    Supported\n",
            "    formats are\n",
            "    those\n",
            "    supported\n",
            "    by torchaud\n",
            "    io.load().\n",
            "    For\n",
            "    example,\n",
            "    wav and\n",
            "    flac are\n",
            "    supported.\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --checkpoint CHECKPOINT\n",
            "    Path to the\n",
            "    checkpoint.\n",
            "    The\n",
            "    checkpoint\n",
            "    is assumed\n",
            "    to be saved\n",
            "    by icefall.\n",
            "    checkpoint.\n",
            "    save_checkp\n",
            "    oint(). You\n",
            "    can use ./t\n",
            "    dnn/export.\n",
            "    py to\n",
            "    obtain it.\n",
            "    (default:\n",
            "    None)\n",
            "  --words-file WORDS_FILE\n",
            "    Path to\n",
            "    words.txt\n",
            "    (default:\n",
            "    None)\n",
            "  --HLG HLG\n",
            "    Path to\n",
            "    HLG.pt.\n",
            "    (default:\n",
            "    None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2zEQaGxwUob"
      },
      "source": [
        "### Decode a single sound file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxMbwNH0s9Sl",
        "outputId": "6061f059-f0a1-4f90-870f-05e7f8b79d22"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/pretrained.py \\\n",
        "    --checkpoint ./tmp/icefall_asr_yesno_tdnn/pretrained.pt \\\n",
        "    --words-file ./tmp/icefall_asr_yesno_tdnn/lang_phone/words.txt \\\n",
        "    --HLG ./tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt \\\n",
        "    ./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-06 05:27:08,084 INFO [pretrained.py:136] {'feature_dim': 23, 'num_classes': 4, 'sample_rate': 8000, 'search_beam': 20, 'output_beam': 8, 'min_active_states': 30, 'max_active_states': 10000, 'use_double_scores': True, 'checkpoint': './tmp/icefall_asr_yesno_tdnn/pretrained.pt', 'words_file': './tmp/icefall_asr_yesno_tdnn/lang_phone/words.txt', 'HLG': './tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt', 'sound_files': ['./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav']}\n",
            "2023-10-06 05:27:08,103 INFO [pretrained.py:142] device: cuda:0\n",
            "2023-10-06 05:27:08,103 INFO [pretrained.py:144] Creating model\n",
            "2023-10-06 05:27:09,749 INFO [pretrained.py:156] Loading HLG from ./tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt\n",
            "2023-10-06 05:27:09,751 INFO [pretrained.py:160] Constructing Fbank computer\n",
            "2023-10-06 05:27:09,752 INFO [pretrained.py:170] Reading sound files: ['./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav']\n",
            "2023-10-06 05:27:09,753 INFO [pretrained.py:176] Decoding started\n",
            "2023-10-06 05:27:11,858 INFO [pretrained.py:212] \n",
            "./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav:\n",
            "NO NO YES NO YES NO NO YES\n",
            "\n",
            "\n",
            "2023-10-06 05:27:11,858 INFO [pretrained.py:214] Decoding Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxT9rP6WwXdO"
      },
      "source": [
        "### Decode multiple sound files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6bYzDehvWZI",
        "outputId": "e527ca71-9583-4842-f460-2249d5a184e1"
      },
      "source": [
        "! export PYTHONPATH=/content/icefall:$PYTHONPATH && \\\n",
        "  cd /content/icefall/egs/yesno/ASR && \\\n",
        "  ./tdnn/pretrained.py \\\n",
        "    --checkpoint ./tmp/icefall_asr_yesno_tdnn/pretrained.pt \\\n",
        "    --words-file ./tmp/icefall_asr_yesno_tdnn/lang_phone/words.txt \\\n",
        "    --HLG ./tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt \\\n",
        "    ./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav \\\n",
        "    ./tmp/icefall_asr_yesno_tdnn/test_waves/1_0_1_1_0_1_1_1.wav"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-06 05:27:52,355 INFO [pretrained.py:136] {'feature_dim': 23, 'num_classes': 4, 'sample_rate': 8000, 'search_beam': 20, 'output_beam': 8, 'min_active_states': 30, 'max_active_states': 10000, 'use_double_scores': True, 'checkpoint': './tmp/icefall_asr_yesno_tdnn/pretrained.pt', 'words_file': './tmp/icefall_asr_yesno_tdnn/lang_phone/words.txt', 'HLG': './tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt', 'sound_files': ['./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav', './tmp/icefall_asr_yesno_tdnn/test_waves/1_0_1_1_0_1_1_1.wav']}\n",
            "2023-10-06 05:27:52,374 INFO [pretrained.py:142] device: cuda:0\n",
            "2023-10-06 05:27:52,374 INFO [pretrained.py:144] Creating model\n",
            "2023-10-06 05:27:54,246 INFO [pretrained.py:156] Loading HLG from ./tmp/icefall_asr_yesno_tdnn/lang_phone/HLG.pt\n",
            "2023-10-06 05:27:54,249 INFO [pretrained.py:160] Constructing Fbank computer\n",
            "2023-10-06 05:27:54,249 INFO [pretrained.py:170] Reading sound files: ['./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav', './tmp/icefall_asr_yesno_tdnn/test_waves/1_0_1_1_0_1_1_1.wav']\n",
            "2023-10-06 05:27:54,251 INFO [pretrained.py:176] Decoding started\n",
            "2023-10-06 05:27:56,384 INFO [pretrained.py:212] \n",
            "./tmp/icefall_asr_yesno_tdnn/test_waves/0_0_1_0_1_0_0_1.wav:\n",
            "NO NO YES NO YES NO NO YES\n",
            "\n",
            "./tmp/icefall_asr_yesno_tdnn/test_waves/1_0_1_1_0_1_1_1.wav:\n",
            "YES NO YES YES NO YES YES YES\n",
            "\n",
            "\n",
            "2023-10-06 05:27:56,385 INFO [pretrained.py:214] Decoding Done\n"
          ]
        }
      ]
    }
  ]
}